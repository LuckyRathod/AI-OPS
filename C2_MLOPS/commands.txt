Copy your code into Google Cloud storage bucket
gsutil ls 

Copy your code from Google cloud storage into Google command line shell
gsutil cp -r gs://tf-flask-app/ .

Get Project value 
gcloud config get-value project

Setting the Project 
gcloud config set project lucky-project 

Create a container - This will store in container registry
gcloud builds submit --tag gcr.io/lucky-project/complaintsapi .

Show last 3 containers and also apply filter 
gcloud builds list --filter complaints --limit 3 

Container Details
gcloud builds describe container_id

Container Logs Details
gcloud build logs container_id

Creating Kubernetes Cluster [ We will only use one node now. Because we will auto scale later]
Make sure you first create service account . Or else your cluster will be deployed as public

gcloud container clusters create complaints-gke --zone "us-west1-b" --machine-type "n1-standard-1"
--num-nodes "1" --service-account luckyrathod-gke@luckyproject.iam.gserviceaccount.com

Deploying Deployment.yaml in Cluster 
kubectl apply -f deployment.yaml

Get all deployments 
kubectl get deployments 

Get all pods
kubectl get pods

Describe pod
kubectl describe pod pod_id

Get pod logs 
kubectl logs pod pod_id

Deploying service.yaml in cluster
kubectl apply -f service.yaml

Get all services
kubectl get services

For Scaling . First we will have to increase cluster size
Increase Kubernetes Cluster Size 
gcloud container clusters resize complaints-gke -num-nodes 3 --zone us-west1-b

List all the clusters container list 
gcloud container clusters list 

Manually Scaling Up
Scale PODS - Deployment - Replicas
kubectl scale deployment complaints --replicas 2 

When we dont know when the traffic might increase
Horizontal Pod Autoscaller [This will automatically increase or decrease pod in response to workload]
kubectl autoscale deployment complaints --max 6 --min 2 --cpu-percent 50

Deploying Continer on Google Cloud Run  : Here we dont have to scale up and down. GCP handles everything
gcloud run deploy container_name --image gcr.io/luckyproject/flask-gke --platform managed --memory 1G

Deploying Flask Application on App Engine [Go wherever your app.yaml files is]
gcloud app create
gcloud app deploy

App Engine vs Cloud Run 
https://cloud.google.com/appengine/migration-center/run/compare-gae-with-run#:~:text=In%20App%20Engine%2C%20idle%20instances,allocation%20to%20CPU%20always%20allocated.


Deploy Application on GKE Autopilot (You will be only charged for pods)
https://cloud.google.com/kubernetes-engine/docs/concepts/autopilot-overview#:~:text=GKE%20Autopilot%20is%20a%20mode,security%2C%20and%20other%20preconfigured%20settings.

- Create a AutoPilot GKE Cluster 
- You will get Cluster connect link 
- Come to gcloud shell paste that link 
- Create the container as you have created earlier 
- Deploy Deployment.yaml and Service.yaml


CI-CD Automated Pipeline
- Clone repo in Google cloud shell
- Follow the Process of Creating Cluster 
- Create Docker,Deployment,Service yaml files
- Create Cloud Build [Similar to Jenkins]
- Run Cloud Build YAML File 
gcloud builds submit --config cloudbuild.yaml

To Integrate Github with GCP . And do Automated CI-CD 
- Setup an Google cloud build in Github for that repo
- Create a Trigger "Push to Main" in GCP Build [Cloud Build - Dashboard - Setup build triggers]

FEAST - Open Source Features Store 
Steps :
Initialize Feature Repository based on Template(Local/AWS/Azure) 
> feast init demostore -t gcp 
  This creates 2 files :
   - driver_repo.py : Here we define structure of Incoming data source (BigQuery) in Feature Store . BigQuery : Customer Dataset - Consist of Transactions 
           - We first define BigQuerySource 
           - Create FeatureView (It is Group of Features)
           - We specify .Which features we need to add from BigQuery 


   - feature_store.yaml - It setups metadata for incoming source (Here we specify that store metadata in gcp cloud storage

> cd demostore 
> feast apply 
(This will register feature defination with feature registrey specfied in feature store which is gcp)

After using above command. DataStore will get created but it will be empty 
Create the df (Add features which you want )
> feast materialize start_date end_Date

Now lets say you have trained and deploy your model . Also your Online store is ready . How we will refer this online store 

> Use get_online_features to do prediction if needed




